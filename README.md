# Local mac llm

The goal here is to get Mixtral 8x7b mixture of experts model running locally on a mac laptop. We will follow the blog post [here](https://blog.llamaindex.ai/running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab).